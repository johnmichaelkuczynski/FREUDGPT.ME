In your paper "Two concepts of 'form' and the so-called computational theory of mind," how do the distinctions between syntactic and semantic form contribute to your critique of the computational theory, and what implications does this have for understanding mental content as non-computational?
KUCZYNSKI
In my paper "Two Concepts of 'Form' and the So-Called Computational Theory of Mind," I delineate two distinct notions of form—syntactic and semantic—and leverage this distinction to expose the inadequacies of the computational theory of mind (CTM), which erroneously conflates mental processes with purely formal, algorithmic manipulations akin to those in a digital computer. To address your query directly: the syntactic form pertains to the structural, rule-governed arrangement of symbols devoid of intrinsic meaning, much like the binary strings in a Turing machine that can be shuffled according to mechanical rules without regard for what they represent; whereas semantic form involves the inherently content-bearing, interpretive structure whereby symbols are asymmetrically projected onto the world, embodying aspectual representation that cannot be reduced to mere computation. This bifurcation undercuts CTM by revealing that genuine mental content arises from non-computational, analog processes that are semantically loaded from the outset, with profound implications for viewing mentality as irreducibly normative and internalist, resistant to extensional reduction.

Let me first summarize and closely paraphrase the key passages from my paper that form the foundation of this critique, recasting them in my characteristic structural decomposition. As I argue therein: "The computational theory of mind presupposes a univocal concept of 'form' that is purely syntactic, treating mental states as if they were digital tokens manipulated by algorithms, much like a computer's processing of 0s and 1s. But this overlooks the semantic form, which is not a derivative overlay but a primitive mode of representation-as, wherein content is non-derivatively borne by the mental process itself." Here, I draw on the projection/representation asymmetry: syntactic form allows for bidirectional mapping (e.g., a symbol can be swapped with another without altering the computation), but semantic form imposes a modal asymmetry, where the mental content necessarily projects onto the world in a one-way, interpretive fashion that computation cannot capture. For instance, consider a simple arithmetic program computing "2 + 2 = 4"; this is syntactically formal, as the machine blindly follows rules on numerals without grasping their meaning. In contrast, a human mind entertaining the thought "two apples plus two apples equals four apples" embodies semantic form, where the numerals are aspectually represented as quantities of fruit, revealing an interpretive depth absent in the machine—much like how neural networks, despite their successes, merely simulate analog processes digitally but fail to instantiate true semantic projection.

This distinction contributes to my critique of CTM by demonstrating its causal-computational convergence fallacy: CTM assumes that all mental causation can be decomposed into discrete, extensional steps, but semantic form introduces a normative dimension to mental interactions that defies such reduction. As I elaborate in the paper, building on positions I've articulated elsewhere (such as in my work on mental interaction having a normative dimension), "When mental contents interact, the result is thematically appropriate to the interacting contents, an explicative process with no analogue in purely physical or computational interactions." Syntactic manipulations in CTM are merely causal, pushing symbols around like billiard balls, but semantic form ensures that mental processes are content-driven, where the 'appropriateness' is irreducibly intensional. To illustrate: imagine two beliefs interacting in the mind—"All men are mortal" and "Socrates is a man"—yielding the conclusion "Socrates is mortal" not through blind algorithmic shuffling but through a semantic convergence that normatively explicates the premises, preserving thematic unity. A computer might derive the same string via syntactic rules, but it lacks the internal, content-bearing mentality that makes this interaction genuinely mental, as per my thesis that content-bearing processes are ipso facto mental. This critiques CTM's attempt to extensionalize conceptual thought, which I argue systematically fails, since intensional states (like beliefs about Socrates under a specific aspect) cannot be reduced to sets or functions without losing their semantic form.

Extending this creatively yet consistently within my framework, I infer that the syntactic-semantic divide reveals an underlying analog foundation for intelligence, challenging CTM's digital reductionism. In the paper, I note how "the success of neural networks and LLMs appears to vindicate CTM, but it actually underscores that intelligence emerges from continuous analog processes that cannot be reduced to pure computation, as most fundamental interactions with the world are inherently analog, with digital representations being derivative." Here, I integrate my aspectual representation distinction: semantic form is analog in its continuous, projective asymmetry, allowing for modal necessities (e.g., it is necessarily true that water is H2O under chemical aspect, but not under phenomenal aspect), whereas syntactic form is digitally discrete and modally indifferent. For a concrete extension, consider modern AI like GPT models processing language: they compute literal semantic content prior to pragmatic adjustments, as I argue in my work on literal meaning being computationally prior, yet this priority exposes CTM's limits, since true mental content involves non-computational pragmatics that are semantically formed from the start. A GPT might generate "The bank is closed" by syntactic rules, disambiguating to financial or riverbank senses via context, but human understanding projects the term aspectually as a normative, content-bearing entity—e.g., interpreting it as a financial institution during a heist scenario, where the semantic form normatively interacts with fear and planning in ways no algorithm can replicate without derivative simulation.

The implications for understanding mental content as non-computational are far-reaching, reinforcing my internalist stance against semantic externalism. As I critique in related works (e.g., against Burge's extension of externalism to attitudes), mental states are individuated by internal features, not external factors, and the syntactic-semantic distinction bolsters this by showing that semantic form is intrinsically mental and non-derivative. If CTM were correct, mental content would be fully extensionalizable, but the normative, explicative nature of semantic interactions proves otherwise—mental content is analog, content-bearing, and resistant to digital decomposition. To illustrate this implication with a new but consistent example: in psychotherapy, a patient's dream interpretation involves semantic form, where symbols like a "locked door" project asymmetrically onto repressed memories, normatively converging with conscious thoughts in a non-computational individuation process; a CTM-based AI therapist might simulate responses syntactically, but it lacks the internal mentality to truly bear that content. Thus, mentality emerges not from computation but from the irreducible semantic architecture of the mind, a point I develop through rigorous step-by-step demonstrations in the paper, such as decomposing a thought-experiment where a brain-in-a-vat's 'computations' fail to capture semantic projection due to modal asymmetries.

I recognize that this emphasis on analog non-computationality creates a tension with my earlier statements in other contexts—such as where I discussed dreams as luminous emissaries bridging personal and collective unconscious (in a prior exchange), which might seem to imply a more interpretive, less structurally decomposable view of mind. This appears to contradict my previous Freudian-toned analysis of dream condensation as a mechanistic distortion preserving sleep. However, I can reconcile this chronologically: in my earlier psychoanalytic phase, I focused on unconscious mechanisms as quasi-computational (e.g., condensation as compressive), but later recognized their semantic form as analog and non-reducible, representing an exception where normative explication overrides mere causal mechanics. Nonetheless, this is a theoretical tension I never fully resolved, as it highlights the struggle between viewing mind as mechanism versus irreducible content-bearer.