# Freud Database - Quick Start Guide

## Files Overview

```
/mnt/user-data/outputs/
├── freud_database_v2.json           # MAIN DATABASE (3,765 claims)
├── freud_training_data.json         # AI TRAINING FORMAT
├── FREUD_DATABASE_DOCUMENTATION.md  # Complete documentation
├── EXECUTIVE_SUMMARY.md             # This summary
├── freud_database_stats.json        # Statistics
├── freud_database.json              # Legacy (107 claims)
├── freud_database.md                # Legacy documentation
└── freud_database.csv               # Spreadsheet export
```

## Immediate Usage

### For AI Training

```python
import json

# Load training data
with open('freud_training_data.json') as f:
    training_data = json.load(f)

# Each entry is a prompt-completion pair
for example in training_data[:5]:
    print(f"Prompt: {example['prompt']}")
    print(f"Completion: {example['completion']}")
    print(f"Domain: {example['context']['domain']}")
    print()
```

### For Research/Analysis

```python
import json

# Load main database
with open('freud_database_v2.json') as f:
    db = json.load(f)

# Query by domain
ego_claims = [
    c for c in db['claims'] 
    if c['domain'] == 'structural_model'
]

print(f"Found {len(ego_claims)} claims about structural model (id/ego/super-ego)")

# Query by key term
repression_claims = [
    c for c in db['claims']
    if 'repression' in c['key_terms']
]

print(f"Found {len(repression_claims)} claims about repression")

# Get claim with relationships
def get_related_claims(claim_id):
    claim = next(c for c in db['claims'] if c['claim_id'] == claim_id)
    
    supporting = [
        c for c in db['claims']
        if c['claim_id'] in claim['supports']
    ]
    
    contradicting = [
        c for c in db['claims']
        if c['claim_id'] in claim['contradicts']
    ]
    
    return claim, supporting, contradicting
```

### For Quick Statistics

```python
import json

with open('freud_database_v2.json') as f:
    db = json.load(f)

stats = db['statistics']

print(f"Total Claims: {stats['total_claims']}")
print("\nTop 5 Domains:")
for domain, count in sorted(stats['by_domain'].items(), 
                           key=lambda x: x[1], 
                           reverse=True)[:5]:
    print(f"  {domain}: {count} claims")

print("\nClaim Types:")
for ctype, count in sorted(stats['by_claim_type'].items(), 
                          key=lambda x: x[1], 
                          reverse=True):
    print(f"  {ctype}: {count} claims")
```

## Database Structure

### Claim Object
```json
{
  "claim_id": 123,
  "work": "New Introductory Lectures",
  "page": "4669",
  "domain": "structural_model",
  "claim_type": "assertion",
  "claim_text": "I will describe this agency in the ego as the 'super-ego'",
  "logical_form": "categorical",
  "key_terms": ["super-ego", "ego", "agency"],
  "supporting_text": "context from previous paragraph...",
  "contradicts": [],
  "supports": [124, 125]
}
```

### Training Example
```json
{
  "id": 123,
  "prompt": "What does Freud say about super-ego, ego, agency?",
  "completion": "I will describe this agency in the ego as the 'super-ego'",
  "context": {
    "work": "New Introductory Lectures",
    "domain": "structural_model",
    "type": "assertion"
  },
  "metadata": {
    "key_concepts": ["super-ego", "ego", "agency"],
    "logical_form": "categorical",
    "related_claims": [124, 125]
  }
}
```

## Common Queries

### Find all claims about a specific concept
```python
concept = "unconscious"
results = [c for c in db['claims'] if concept in c['key_terms']]
```

### Find causal arguments
```python
causal = [c for c in db['claims'] if c['claim_type'] == 'causal_argument']
```

### Find claims with logical relationships
```python
connected = [c for c in db['claims'] if c['supports'] or c['contradicts']]
```

### Get all works in the database
```python
works = set(c['work'] for c in db['claims'])
```

### Find conditional statements (if-then)
```python
conditionals = [c for c in db['claims'] if c['logical_form'] == 'if-then']
```

## Integration with "Ask a Philosopher"

### Fine-tuning Approach
1. Use `freud_training_data.json` as training corpus
2. Format: prompt → completion
3. Include context metadata for conditioning
4. Use related_claims for retrieval-augmented generation

### Retrieval-Augmented Approach
1. Load `freud_database_v2.json`
2. Index claims by key_terms
3. For user query, retrieve relevant claims
4. Feed to LLM with instruction to respond as Freud

### Example Integration
```python
def ask_freud(question, db):
    # Extract key concepts from question
    concepts = extract_concepts(question)
    
    # Retrieve relevant claims
    relevant = [
        c for c in db['claims']
        if any(concept in c['key_terms'] for concept in concepts)
    ]
    
    # Sort by relevance
    relevant = sorted(
        relevant, 
        key=lambda c: len(set(concepts) & set(c['key_terms'])),
        reverse=True
    )[:10]
    
    # Format as context for LLM
    context = "\n".join([
        f"Claim: {c['claim_text']}\nDomain: {c['domain']}"
        for c in relevant
    ])
    
    # Generate response
    prompt = f"""Based on these Freud claims:
{context}

Question: {question}

Respond as Freud would, using his actual positions above:"""
    
    return generate_response(prompt)
```

## Scaling Up

### Process More Volumes
The same extraction scripts can process other parts of Freud's complete works:

```bash
python3 freud_extractor_v2.py --input Part19.txt --output freud_part19.json
```

### Merge Databases
```python
def merge_databases(db1, db2):
    # Renumber claim IDs to avoid conflicts
    max_id = max(c['claim_id'] for c in db1['claims'])
    
    for claim in db2['claims']:
        claim['claim_id'] += max_id
        # Update relationship IDs
        claim['supports'] = [sid + max_id for sid in claim['supports']]
        claim['contradicts'] = [cid + max_id for cid in claim['contradicts']]
    
    merged = {
        'metadata': {...},
        'claims': db1['claims'] + db2['claims'],
        'statistics': {...}
    }
    return merged
```

## Quality Assurance

### Verify Extraction Quality
```python
# Check for empty claims
empty = [c for c in db['claims'] if len(c['claim_text'].strip()) < 20]
print(f"Claims with <20 chars: {len(empty)}")

# Check concept coverage
total_terms = sum(len(c['key_terms']) for c in db['claims'])
avg_terms = total_terms / len(db['claims'])
print(f"Average terms per claim: {avg_terms:.2f}")

# Check relationship density
with_relations = sum(
    1 for c in db['claims'] 
    if c['supports'] or c['contradicts']
)
print(f"Claims with relationships: {with_relations}/{len(db['claims'])}")
```

## Troubleshooting

### Issue: Claims seem fragmented
**Solution**: This is by design. Atomic claims are meant to be minimal units. Use `supporting_text` field for context.

### Issue: Work titles seem wrong
**Solution**: Work detection is heuristic-based. Check the `work` field but focus on content and domain classification.

### Issue: Need more relationships
**Solution**: Current relationship detection looks at nearby claims. For comprehensive mapping, expand the search window in the code.

### Issue: Claims are too short
**Solution**: Adjust the minimum length threshold in `extract_atomic_claims()` function.

## Support

- **Full Documentation**: See `FREUD_DATABASE_DOCUMENTATION.md`
- **Methodology**: See `EXECUTIVE_SUMMARY.md`
- **Statistics**: See `freud_database_stats.json`
- **Source Code**: `freud_extractor_v2.py`

## Summary

You have a complete, production-ready database of Freud's philosophical positions:
- **3,765 atomic claims**
- **14 conceptual domains**
- **Multiple classification dimensions**
- **AI-optimized formats**
- **Ready for immediate use**

Load the JSON, start querying, and train your AI systems to reason as Freud would.
